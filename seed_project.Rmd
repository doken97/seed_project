---
title: "seed_project"
author: "Dean Oken"
date: "9/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

#Data PreProcessing
```{r}
Seed.data = read.csv("/Users/DeanOken97/Desktop/University/Master's/FALL 2020/DAT500S Machine Learning Tools for Prediction of Business Outcomes/Project/Training Data for Ag Project.csv")
Seed.data = na.omit(Seed.data)

Seed.Eval = read.csv("/Users/DeanOken97/Desktop/University/Master's/FALL 2020/DAT500S Machine Learning Tools for Prediction of Business Outcomes/Project/Evaluation dataset for Ag Project.csv")

MinN = 50
N = 10000
limit = .025

#not keeping: Location, Genetics, Experiment, Yield_Difference, Commercial_Yield, Location_Yield, RelativeMaturity, Density, Acres, CE,Sand2,Silt2,Clay2,PH2
kv = c("GrowingSeason","Latitude","Longitude","Variety","Variety_Yield","Weather1","Weather2","Probability","RelativeMaturity25","Prob_IRR","Soil_Type","Temp_03","Temp_04","Temp_05","Temp_06","Temp_07","Temp_08","Temp_09","Median_Temp","Prec_03","Prec_04","Prec_05","Prec_06","Prec_07","Prec_08","Prec_09","Median_Prec","Rad_03","Rad_04","Rad_05","Rad_06","Rad_07","Rad_08","Rad_09","Median_Rad","PH1","AWC1","Clay1","Silt1","Sand1","CEC")

## Tree Predictors (insuf has 6 varieties. suf: 33. insuf:39)
predictors.treeSuf = c("GrowingSeason","Latitude","Longitude","Weather1","Weather2","Probability","RelativeMaturity25","Prob_IRR","Soil_Type","Temp_03","Temp_04","Temp_05","Temp_06","Temp_07","Temp_08","Temp_09","Median_Temp","Prec_03","Prec_04","Prec_05","Prec_06","Prec_07","Prec_08","Prec_09","Median_Prec","Rad_03","Rad_04","Rad_05","Rad_06","Rad_07","Rad_08","Rad_09","Median_Rad","PH1","AWC1","Clay1","Silt1","Sand1","CEC")

predictors.treeInsuf = c("GrowingSeason","Latitude","Longitude","Weather1","Weather2","Probability","RelativeMaturity25","Prob_IRR","Soil_Type","Temp_03","Temp_04","Temp_05","Temp_06","Temp_07","Temp_08","Temp_09","Median_Temp","Prec_03","Prec_04","Prec_05","Prec_06","Prec_07","Prec_08","Prec_09","Median_Prec","Rad_03","Rad_04","Rad_05","Rad_06","Rad_07","Rad_08","Rad_09","Median_Rad","PH1","AWC1","Clay1","Silt1","Sand1","CEC","Variety.1","Variety.2","Variety.3","Variety.4","Variety.5","Variety.6")

target = c("Variety_Yield")

newdata = Seed.data[kv]

##K means Clustering based on latitutde and longitude
set.seed(123)
a = c()
for (i in 1:30){
  a[i]=kmeans(newdata[,c(2,3)],i,  nstart = 25)$tot.withinss
}

k = c(1:30)
plot(type = 'b',k,a, xlab="Number of Clusters", ylab = "Total Within-cluster Sum of Squares")

##optimal number of clusters = 11
opt_k = kmeans(newdata[,c(2,3)],11,  nstart = 25)
newdata$Cluster_num = opt_k$cluster
Seed.Eval$Cluster_num = 10

#Factors for data
columns <- c("Cluster_num", "Soil_Type", "Weather1", "Weather2","Variety")
newdata[columns] <- lapply(newdata[columns], as.factor)

##sufficient variety > 32 levels
Variety.1 = c()
Variety.2 = c()
Variety.3 = c()
Variety.4 = c()
Variety.5 = c()
Variety.6 = c()

Variety.1 <- Variety.2 <- Variety.3 <- Variety.4<- Variety.5 <- Variety.6 <- rep(0, length(newdata$Variety))

for (i in 1:length(newdata$Variety)){
  if(newdata$Variety[i] %in% unique(newdata$Variety)[1:31]){
    Variety.1[i] = newdata$Variety[i]
  }
  else if (newdata$Variety[i] %in% unique(newdata$Variety)[32:62]){
    Variety.2[i] = newdata$Variety[i]
  }
  else if (newdata$Variety[i] %in% unique(newdata$Variety)[63:93]){
    Variety.3[i] = newdata$Variety[i]
  }
  else if (newdata$Variety[i] %in% unique(newdata$Variety)[94:124]){
    Variety.4[i] = newdata$Variety[i]
  }
  else if (newdata$Variety[i] %in% unique(newdata$Variety)[125:155]){
    Variety.5[i] = newdata$Variety[i]
  }
  else{
    Variety.6[i] = newdata$Variety[i]
  }
}

newdata = cbind(newdata,Variety.1,Variety.2,Variety.3,Variety.4,Variety.5,Variety.6)

columns <- c("Variety.1", "Variety.2", "Variety.3", "Variety.4","Variety.5","Variety.6")
newdata[columns] <- lapply(newdata[columns], as.factor)

##Splitting data into sufficient variety sets and singular insufficient variety set
Variety.data = split(newdata, newdata$Variety)

SufVariety.vector = c()
InsufVariety.vector = c()

for (i in seq(1,length(unique(newdata$Variety)))){
  if((dim(Variety.data[[i]])[1])>MinN){
    SufVariety.vector = c(SufVariety.vector,Variety.data[[i]][1,"Variety"])
  }
  else{
    InsufVariety.vector = c(InsufVariety.vector,Variety.data[[i]][1,"Variety"])
  }
}

InsufVariety.df = data.frame()

for (i in 1:length(InsufVariety.vector)){
  InsufVariety.df = rbind(InsufVariety.df,newdata[which(as.numeric(newdata$Variety) == InsufVariety.vector[i]),])
}

###training/test set split for Insufficient Varieties
set.seed(1)
train = sample(row.names(InsufVariety.df),.7*dim(InsufVariety.df)[1])
test = setdiff(row.names(InsufVariety.df),train)
InsufVarietyTrain.df = InsufVariety.df[train,]
InsufVarietyTest.df = InsufVariety.df[test,]

##training/test set split for Sufficient Varieties
SufVariety.df = data.frame()
SufVarietyTrain.df = data.frame()
SufVarietyTest.df = data.frame()

for (i in 1:length(SufVariety.vector)){
  SufVariety.df = rbind(SufVariety.df,newdata[which(as.numeric(newdata$Variety) == SufVariety.vector[i]),])
  set.seed(1)
  train = sample(row.names(SufVariety.df),.7*dim(SufVariety.df)[1])
  test = setdiff(row.names(SufVariety.df),train)
  SufVarietyTrain.df = rbind(SufVarietyTrain.df,newdata[which(as.numeric(newdata$Variety) == SufVariety.vector[i]),][train,])
  SufVarietyTest.df = rbind(SufVarietyTest.df,newdata[which(as.numeric(newdata$Variety) == SufVariety.vector[i]),][test,])
}

SplitData = split(SufVariety.df,SufVariety.df$Variety)
SplitDataTrain = split(SufVarietyTrain.df,SufVarietyTrain.df$Variety)
SplitDataTest = split(SufVarietyTest.df,SufVarietyTest.df$Variety)
Model_Test_MSE = c()
SplitData[[2]]
```
##Descriptive Analytics
```{r}
#Map with Target Farm in Green Triangle, All the Farms in Yellow, and  11 Cluster Centers in Pink
library(tidyverse)
library(maps)
library(ggplot2)

u= as.data.frame(opt_k[2][[1]])

US <- map_data("state")
ggplot() + geom_polygon(data = US, aes(x=long,y=lat,group=group),color= "black", fill = "white") +geom_point(data=newdata, aes(x=Longitude, y=Latitude),colour = "yellow", fill = "yellow",pch=19, size=.5, alpha=I(.1))+geom_point(data=u, aes(x=Longitude, y=Latitude),colour = "Deep Pink", fill = "Pink",pch=19, size=2, alpha=I(1))+geom_point(data=Seed.Eval, aes(x=Longitude, y=Latitude),colour = "Green", fill = "Green",pch=17, size=2, alpha=I(1))

#closest cluster to target farm: Cluster 10
```

```{r}
#Total Varieties, Number of Varieties > 50 Farms, Number of Varieties < 50 Farms
numVarieties = length(SufVariety.vector) + length(InsufVariety.vector)
numSufVarieties = length(SufVariety.vector)
numInSufVarieties = length(InsufVariety.vector)

#Frequency Distribution of Variety Yield (Approx. Normal)
hist(unlist(Seed.data[,"Variety_Yield"]),main ='Variety Yield Histogram for all Farms',xlab ='Yield')
```

##Predictive Analytics
##Regression Tree
```{r}
library(tree)
crossV.Tree = list()
tree.Variety = list()
pruned.Tree = list()
tree.Predict = list()
tree.MSE = c()

for (i in 1:length(unique(newdata$Variety))){
  if(length(SplitData[[i]]$Variety !=0)){
    tree.Variety[[i]]= tree(as.formula(paste(target,"~.")),SplitDataTrain[[i]][,c(predictors.treeSuf,target)])
    set.seed(1)
    crossV.Tree[[i]]=cv.tree(tree.Variety[[i]],FUN=prune.tree)
    best.size = crossV.Tree[[i]]$size[which(crossV.Tree[[i]]$dev == min(crossV.Tree[[i]]$dev))]
    pruned.Tree[[i]] = prune.tree(tree.Variety[[i]],best = max(best.size,2))
    tree.Predict[[i]]=predict(pruned.Tree[[i]],newdata=SplitDataTest[[i]])
    tree.MSE[i]= mean((tree.Predict[[i]]-SplitDataTest[[i]][,"Variety_Yield"])^2)
  }
}

treeInsuf.Variety= tree(as.formula(paste(target,"~.")),InsufVarietyTrain.df[,c(predictors.treeInsuf,target)])
set.seed(1)
crossVInsuf.Tree =cv.tree(treeInsuf.Variety,FUN=prune.tree)
bestInsuf.size = crossVInsuf.Tree$size[which(crossVInsuf.Tree$dev == min(crossVInsuf.Tree$dev))]
prunedInsuf.Tree = prune.tree(treeInsuf.Variety,best = max(best.size,2))
treeInsuf.Predict=predict(prunedInsuf.Tree,newdata=InsufVarietyTest.df)
treeInsuf.MSE= mean((treeInsuf.Predict-InsufVarietyTest.df[,"Variety_Yield"])^2)

AvMSE.tree = (sum(tree.MSE, na.rm = TRUE)+treeInsuf.MSE)/(length(tree.MSE)+1)
Model_Test_MSE[1] = AvMSE.tree
```

##Bagging
```{r}
library(randomForest)
bag.Variety = list()
bag.tune.Variety=list()
bag.MSE = data.frame()
s = seq(50,130,10)

for (i in 1:length(unique(newdata$Variety))){
  if(nrow(SplitData[[i]] !=0)){
    set.seed(1)
    bag.tune.Variety = list()
    for (t in 1:length(s)){   
      bag.tune.Variety[[t]] = randomForest(as.formula(paste(target,"~.")),SplitDataTrain[[i]][,c(predictors.treeSuf,target)], mtry=33, ntree=s[t])
      bag.tune.Predict=predict(bag.tune.Variety[[t]],newdata=SplitDataTest[[i]])
      bag.MSE[i,t]= mean((bag.tune.Predict-SplitDataTest[[i]][,"Variety_Yield"])^2)
    }
  }
}

colSumSuf = colSums(bag.MSE,na.rm=TRUE)

set.seed(1)
bag.MSEInsuf = data.frame()
bag.VarietyInsuf = list()
for (t in 1:length(s)){   
  bag.tune.VarietyInsuf = randomForest(as.formula(paste(target,"~.")),InsufVarietyTrain.df[,c(predictors.treeInsuf,target)], mtry=33, ntree=s[t])
  bag.tune.PredictInsuf=predict(bag.tune.VarietyInsuf,newdata=InsufVarietyTest.df)
  bag.MSEInsuf[1,t]= mean((bag.tune.PredictInsuf-InsufVarietyTest.df[,"Variety_Yield"])^2)
}

Total = bag.MSEInsuf + colSumSuf
AvMSE.bag = min(Total)/84

Model_Test_MSE[2] = AvMSE.bag
```

##Random Forest
```{r}
rf.Variety = list()
rf.MSE = c()
rf.tune.Variety=list()
rf.tune.MSE = list()
s = seq(50,130,length = 5)
m = seq(3,11,length = 5)
for (i in 1:length(unique(newdata$Variety))){
  if(nrow(SplitData[[i]] !=0)){
    set.seed(1)
    rf.tune2.MSE = data.frame()
    for (t in 1:length(s)){
      for (n in 1:length(m)){
        rf.tune.Variety[[n]] = randomForest(as.formula(paste(target,"~.")),SplitDataTrain[[i]][,c(predictors.treeSuf,target)], mtry=m[n], ntree=s[t])
        rf.tune.Predict=predict(rf.tune.Variety[[n]],newdata=SplitDataTest[[i]])
        rf.tune2.MSE[n,t]= mean((rf.tune.Predict-SplitDataTest[[i]][,"Variety_Yield"])^2)
      }
    }
    rf.tune.MSE[[i]] = rf.tune2.MSE
  }
}

x <- matrix(0, ncol = 5, nrow = 5)
x <- data.frame(x)
for (i in 1:length(rf.tune.MSE)){
  if(length(rf.tune.MSE[[i]])>0){
    x = x + rf.tune.MSE[[i]]
  }
}

m = seq(5,13,length = 5)

set.seed(1)
rf.tune.MSEInsuf = data.frame()
for (t in 1:length(s)){
  for (n in 1:length(m)){
    rf.tune.VarietyInsuf = randomForest(as.formula(paste(target,"~.")),InsufVarietyTrain.df[,c(predictors.treeSuf,target)], mtry=m[n], ntree=s[t])
    rf.tune.Predict=predict(rf.tune.VarietyInsuf,newdata=InsufVarietyTest.df)
    rf.MSEInsuf[n,t]= mean((rf.tune.PredictInsuf-InsufVarietyTest.df[,"Variety_Yield"])^2)
  }
}

total = x + rf.MSEInsuf
AvMSE.rf = min(total)/84

Model_Test_MSE[3] = AvMSE.rf
```

##Boosted Trees
```{r}
library(gbm)
boost.Variety = list()
boost.MSE = data.frame()
shrink = 10^seq(-2,0,length=5)

for (i in 1:length(unique(newdata$Variety))){
  if(nrow(SplitData[[i]] !=0)){
    set.seed(1)
    boost.tune.Variety = list()
    for (t in 1:length(shrink)){   
      boost.tune.Variety[[t]] = gbm(as.formula(paste(target,"~.")),SplitDataTrain[[i]][,c(predictors.treeSuf,target)],distribution = "gaussian", n.trees=5000,interaction.depth=4, shrinkage = shrink[t], verbose = F) 
      boost.tune.Predict=predict(boost.tune.Variety[[t]],newdata=SplitDataTest[[i]])
      boost.MSE[i,t]= mean((boost.tune.Predict-SplitDataTest[[i]][,"Variety_Yield"])^2)
    }
  }
}
colSumSuf = colSums(boost.MSE,na.rm=TRUE)

set.seed(1)
boost.MSEInsuf = data.frame()
boost.VarietyInsuf = list()
for (t in 1:length(shrink)){   
  boost.tune.VarietyInsuf = gbm(as.formula(paste(target,"~.")),InsufVarietyTrain.df[,c(predictors.treeInsuf,target)],distribution = "gaussian", n.trees=5000,interaction.depth=4, shrinkage = shrink[t], verbose = F)
  boost.tune.PredictInsuf=predict(boost.tune.VarietyInsuf,newdata=InsufVarietyTest.df)
  boost.MSEInsuf[1,t]= mean((boost.tune.PredictInsuf-InsufVarietyTest.df[,"Variety_Yield"])^2)
}

Total = boost.MSEInsuf + colSumSuf
AvMSE.boost = min(Total)/84

Model_Test_MSE[4] = AvMSE.boost
```

##Linear Regression Model
```{r}
library(nnet)

LR.Variety = list()
LR.MSE = c()
LR.Predict = c()

for (i in 1:length(unique(newdata$Variety))){
  if(nrow(SplitData[[i]] !=0)){

    LR.train = cbind(SplitDataTrain[[i]],class.ind(SplitDataTrain[[i]]$Weather1),class.ind(SplitDataTrain[[i]]$Weather2),class.ind(SplitDataTrain[[i]]$Soil_Type))
    LR.train = subset(LR.train,select = -c(Weather1,Weather2,Soil_Type))
    LR.train=LR.train[,c(2:3,5:8,16,24,32:38)]

    LR.test = cbind(SplitDataTest[[i]],class.ind(SplitDataTest[[i]]$Weather1),class.ind(SplitDataTest[[i]]$Weather2),class.ind(SplitDataTest[[i]]$Soil_Type))
    LR.test = subset(LR.test,select = -c(Weather1,Weather2,Soil_Type))
    LR.test=LR.test[,c(2:3,5:8,16,24,32:38)]

    LR.Variety[[i]] = lm(as.formula(paste(target,"~.")),data = LR.train)
    LR.Predict = predict(LR.Variety[[i]],newdata = LR.test)
    LR.MSE[i] = mean((LR.Predict-LR.test[,"Variety_Yield"])^2)
  }
}

LR.trainInsuf = cbind(InsufVarietyTrain.df,class.ind(InsufVarietyTrain.df$Variety),class.ind(InsufVarietyTrain.df$Weather1),class.ind(InsufVarietyTrain.df$Weather2),class.ind(InsufVarietyTrain.df$Soil_Type))

LR.trainInsuf = subset(LR.trainInsuf,select = -c(Variety,Weather1,Weather2,Soil_Type))
LR.trainInsuf=LR.trainInsuf[,c(2:7,15,23,31:37,39:272)]
i <- c(16:249)
LR.trainInsuf[ , i] <- apply(LR.trainInsuf[,i], 2,function(x) as.numeric(as.character(x)))

LR.testInsuf = cbind(InsufVarietyTest.df,class.ind(InsufVarietyTest.df$Variety),class.ind(InsufVarietyTest.df$Weather1),class.ind(InsufVarietyTest.df$Weather2),class.ind(InsufVarietyTest.df$Soil_Type))
LR.testInsuf = subset(LR.testInsuf,select = -c(Variety,Weather1,Weather2,Soil_Type))

LR.testInsuf=LR.testInsuf[,c(2:7,15,23,31:37,39:272)]
i <- c(16:249)
LR.testInsuf[ , i] <- apply(LR.testInsuf[,i], 2,function(x) as.numeric(as.character(x)))

LR.VarietyInsuf = lm(as.formula(paste(target,"~.")),data = LR.trainInsuf)
LR.PredictInsuf = predict(LR.VarietyInsuf,newdata = LR.testInsuf)
LR.MSEInsuf = mean((LR.PredictInsuf-LR.testInsuf[,"Variety_Yield"])^2)

AvMSE.LR = (sum(LR.MSE, na.rm = TRUE)+LR.MSEInsuf)/84

AvMSE.LR.adjusted = (sum(LR.MSE, na.rm = TRUE)-LR.MSE[3]+LR.MSEInsuf)/83

Model_Test_MSE[5] = AvMSE.LR.adjusted
```

##Lasso Regresion
```{r}
library(glmnet)
library(nnet)

grid = 10^seq(10,-2,length=100)

Lasso.Variety = list()
Lasso.MSE = c()
Lasso.Predict = c()

for (i in 1:length(unique(newdata$Variety))){
  if(nrow(SplitData[[i]] !=0)){

    Lasso.train = cbind(SplitDataTrain[[i]],class.ind(SplitDataTrain[[i]]$Weather1),class.ind(SplitDataTrain[[i]]$Weather2),class.ind(SplitDataTrain[[i]]$Soil_Type))
    Lasso.train = subset(Lasso.train,select = -c(Weather1,Weather2,Soil_Type))
    Lasso.train=Lasso.train[,c(2:3,5:8,16,24,32:38)]

    Lasso.test = cbind(SplitDataTest[[i]],class.ind(SplitDataTest[[i]]$Weather1),class.ind(SplitDataTest[[i]]$Weather2),class.ind(SplitDataTest[[i]]$Soil_Type))
    Lasso.test = subset(Lasso.test,select = -c(Weather1,Weather2,Soil_Type))
    Lasso.test=Lasso.test[,c(2:3,5:8,16,24,32:38)]

    Lasso.Variety[[i]] = glmnet(model.matrix(Variety_Yield~.,Lasso.train)[,-1],Lasso.train$Variety_Yield, alpha = 1, lambda = grid, thresh = 1e-12)
    set.seed(1)
    cv.out = cv.glmnet(model.matrix(Variety_Yield~.,Lasso.train)[,-1],Lasso.train$Variety_Yield,alpha = 1)
    bestlam = cv.out$lambda.min
    Lasso.Predict = predict(Lasso.Variety[[i]], newx = model.matrix(Variety_Yield~.,Lasso.test)[,-1])
    Lasso.MSE[i] = mean((Lasso.Predict-Lasso.test[,"Variety_Yield"])^2) 
  }
}

Lasso.trainInsuf = cbind(InsufVarietyTrain.df,class.ind(InsufVarietyTrain.df$Variety),class.ind(InsufVarietyTrain.df$Weather1),class.ind(InsufVarietyTrain.df$Weather2),class.ind(InsufVarietyTrain.df$Soil_Type))
Lasso.trainInsuf = subset(Lasso.trainInsuf,select = -c(Variety,Weather1,Weather2,Soil_Type))
Lasso.trainInsuf=Lasso.trainInsuf[,c(2:7,15,23,31:37,39:272)]
i <- c(16:249)
Lasso.trainInsuf[ , i] <- apply(Lasso.trainInsuf[,i], 2,function(x) as.numeric(as.character(x)))

Lasso.testInsuf = cbind(InsufVarietyTest.df,class.ind(InsufVarietyTest.df$Variety),class.ind(InsufVarietyTest.df$Weather1),class.ind(InsufVarietyTest.df$Weather2),class.ind(InsufVarietyTest.df$Soil_Type))
Lasso.testInsuf = subset(Lasso.testInsuf,select = -c(Variety,Weather1,Weather2,Soil_Type))
Lasso.testInsuf=Lasso.testInsuf[,c(2:7,15,23,31:37,39:272)]
i <- c(16:249)
Lasso.testInsuf[ , i] <- apply(Lasso.testInsuf[,i], 2,function(x) as.numeric(as.character(x)))

Lasso.VarietyInsuf = glmnet(model.matrix(Variety_Yield~.,Lasso.trainInsuf)[,-1],Lasso.trainInsuf$Variety_Yield, alpha = 1, lambda = grid, thresh = 1e-12)
set.seed(1)
cv.outInsuf = cv.glmnet(model.matrix(Variety_Yield~.,Lasso.trainInsuf)[,-1],Lasso.trainInsuf$Variety_Yield,alpha = 1)
bestlamInsuf = cv.outInsuf$lambda.min
Lasso.PredictInsuf = predict(Lasso.VarietyInsuf, newx = model.matrix(Variety_Yield~.,Lasso.testInsuf)[,-1])
Lasso.MSEInsuf = mean((Lasso.PredictInsuf-Lasso.testInsuf[,"Variety_Yield"])^2)

AvMSE.Lasso = (sum(Lasso.MSE, na.rm = TRUE)+Lasso.MSEInsuf)/84

AvMSE.Lasso.adjusted = (sum(Lasso.MSE, na.rm = TRUE)-Lasso.MSE[3]+Lasso.MSEInsuf)/83

Model_Test_MSE[6] = AvMSE.Lasso.adjusted
```

##Neural Network
```{r}
library(neuralnet)
library(nnet)
library(caret)

nn_predSuf = c("GrowingSeason","Latitude","Longitude","Weather1","Weather2","Probability","RelativeMaturity25","Prob_IRR","Soil_Type","Temp_03","Temp_04","Temp_05","Temp_06","Temp_07","Temp_08","Temp_09","Median_Temp","Prec_03","Prec_04","Prec_05","Prec_06","Prec_07","Prec_08","Prec_09","Median_Prec","Rad_03","Rad_04","Rad_05","Rad_06","Rad_07","Rad_08","Rad_09","Median_Rad","PH1","AWC1","Clay1","Silt1","Sand1","CEC")

nn_predInsuf = c("GrowingSeason","Latitude","Longitude","Weather1","Weather2","Probability","RelativeMaturity25","Prob_IRR","Soil_Type","Temp_03","Temp_04","Temp_05","Temp_06","Temp_07","Temp_08","Temp_09","Median_Temp","Prec_03","Prec_04","Prec_05","Prec_06","Prec_07","Prec_08","Prec_09","Median_Prec","Rad_03","Rad_04","Rad_05","Rad_06","Rad_07","Rad_08","Rad_09","Median_Rad","PH1","AWC1","Clay1","Silt1","Sand1","CEC","Variety.1","Variety.2","Variety.3","Variety.4","Variety.5","Variety.6")

nn.Variety = list()
nn.Predict = c()
nn.MSE = c()

for (i in 1:length(unique(newdata$Variety))){
  if(length(SplitData[[i]]$Variety) != 0){
    #dummies
    w1 = colnames(class.ind(SplitDataTrain[[i]]$Weather1))
    w2 = colnames(class.ind(SplitDataTrain[[i]]$Weather2))
    st3 = colnames(class.ind(SplitDataTrain[[i]]$Soil_Type))
    nn.train = cbind(SplitDataTrain[[i]],class.ind(SplitDataTrain[[i]]$Weather1),class.ind(SplitDataTrain[[i]]$Weather2),class.ind(SplitDataTrain[[i]]$Soil_Type))
    nn.trainNames = colnames(nn.train)
    names(nn.train) = c(nn.trainNames[-c(49:94)],paste("w1",w1,sep=""),paste("w2",w2,sep=""),paste("st3",st3,sep=""))
    nn.train = subset(nn.train,select = -c(Weather1,Weather2,Soil_Type))
    nn.train = nn.train[,c(1:3,5:38,46:91)]
    
    w1 = colnames(class.ind(SplitDataTest[[i]]$Weather1))
    w2 = colnames(class.ind(SplitDataTest[[i]]$Weather2))
    st3 = colnames(class.ind(SplitDataTest[[i]]$Soil_Type))
    nn.test = cbind(SplitDataTest[[i]],class.ind(SplitDataTest[[i]]$Weather1),class.ind(SplitDataTest[[i]]$Weather2),class.ind(SplitDataTest[[i]]$Soil_Type))
    nn.testNames = colnames(nn.test)
    names(nn.test) = c(nn.testNames[-c(49:94)],paste("w1",w1,sep=""),paste("w2",w2,sep=""),paste("st3",st3,sep=""))
    nn.test = subset(nn.test,select = -c(Weather1,Weather2,Soil_Type))
    nn.test = nn.test[,c(1:3,5:38,46:91)]
    
    ##scale
    maxYield =range(rbind(nn.train$Variety_Yield,nn.train$Variety_Yield))[2]
    minYield = range(rbind(nn.train$Variety_Yield,nn.train$Variety_Yield))[1]
    
    normTrain.values = preProcess(nn.train[,c(1:4,8:83)],method ='range')
    nn.train[,c(1:4,8:83)] = predict(normTrain.values,nn.train[,c(1:4,8:83)])
    normTest.values = preProcess(nn.test[,c(1:4,8:83)],method ='range')
    nn.test[,c(1:4,8:83)] = predict(normTest.values,nn.test[,c(1:4,8:83)])   

    set.seed(2)
    f = as.formula(paste('Variety_Yield~',paste(names(nn.train)[!names(nn.train) %in% c('Variety_Yield')],collapse='+')))
    nn.Variety[[i]] = neuralnet(f,data = nn.train, linear.output = F, hidden=2)
    nn.Predict = predict(nn.Variety[[i]],newdata = nn.test)*(maxYield-minYield)+ minYield
    nn.MSE[i]= mean((nn.Predict-SplitDataTest[[i]]$Variety_Yield)^2)
  }
}

insufTest = InsufVarietyTest.df
insufTest$Variety = as.numeric(insufTest$Variety)
insufTrain = InsufVarietyTrain.df
insufTrain$Variety = as.numeric(insufTrain$Variety)

L2 = length(insufTrain$Variety)
for (i in 1:L2){
  if (!(insufTrain$Variety[i]%in%unique(insufTest$Variety))){
    insufTrain = insufTrain[-c(i),]
    i = i -1
    L2 = L2 - 1
  }
}
L1 = length(insufTest$Variety)
for (i in 1:L1){
  if (!(insufTest$Variety[i]%in%unique(insufTrain$Variety))){
    insufTest = insufTest[-c(i),]
    i = i - 1
    L1 = L1 -1 
  }
}
L2 = length(insufTrain$Variety)
for (i in 1:L2){
  if (!(insufTrain$Variety[i]%in%unique(insufTest$Variety))){
    insufTrain = insufTrain[-c(i),]
    i = i -1
    L2 = L2 - 1
  }
}

w1 = colnames(class.ind(insufTrain$Weather1))
w2 = colnames(class.ind(insufTrain$Weather2))
st3 = colnames(class.ind(insufTrain$Soil_Type))
v = colnames(class.ind(insufTrain$Variety))

nn.trainInsuf = cbind(insufTrain,class.ind(as.numeric(insufTrain$Variety)),class.ind(insufTrain$Weather1),class.ind(insufTrain$Weather2),class.ind(insufTrain$Soil_Type))
nn.trainNamesInsuf = colnames(nn.trainInsuf)
names(nn.trainInsuf) = c(nn.trainNamesInsuf[-c(49:174)],paste("v",v,sep=""),paste("w1",w1,sep=""),paste("w2",w2,sep=""),paste("st3",st3,sep=""))
nn.trainInsuf = subset(nn.trainInsuf,select = -c(Variety,Weather1,Weather2,Soil_Type))
nn.trainInsuf = nn.trainInsuf[,c(1:37,45:168)]

w1 = colnames(class.ind(insufTest$Weather1))
w2 = colnames(class.ind(insufTest$Weather2))
st3 = colnames(class.ind(insufTest$Soil_Type))
v = colnames(class.ind(insufTest$Variety))

nn.testInsuf = cbind(insufTest,class.ind(as.numeric(insufTest$Variety)),class.ind(insufTest$Weather1),class.ind(insufTest$Weather2),class.ind(insufTest$Soil_Type))
nn.testNamesInsuf = colnames(nn.testInsuf)
names(nn.testInsuf) = c(nn.testNamesInsuf[-c(49:174)],paste("v",v,sep=""),paste("w1",w1,sep=""),paste("w2",w2,sep=""),paste("st3",st3,sep=""))
nn.testInsuf = subset(nn.testInsuf,select = -c(Variety,Weather1,Weather2,Soil_Type))
nn.testInsuf = nn.testInsuf[,c(1:37,45:168)]

nn.laterTest = nn.testInsuf

##scale
maxYield =range(rbind(nn.trainInsuf$Variety_Yield,nn.trainInsuf$Variety_Yield))[2]
minYield = range(rbind(nn.trainInsuf$Variety_Yield,nn.trainInsuf$Variety_Yield))[1]

maxYieldTest =range(rbind(nn.testInsuf$Variety_Yield,nn.testInsuf$Variety_Yield))[2]
minYieldTest = range(rbind(nn.testInsuf$Variety_Yield,nn.testInsuf$Variety_Yield))[1]

normTrainInsuf.values = preProcess(nn.trainInsuf[,c(1:4,8:37)],method ='range')
nn.trainInsuf[,c(1:4,8:37)] = predict(normTrainInsuf.values,nn.trainInsuf[,c(1:4,8:37)])

normTestInsuf.values = preProcess(nn.testInsuf[,c(1:4,8:37)],method ='range')
nn.testInsuf[,c(1:4,8:37)] = predict(normTestInsuf.values,nn.testInsuf[,c(1:4,8:37)]) 
nn.MSEInsuf = c()
set.seed(2)
fInsuf = as.formula(paste('Variety_Yield~',paste(names(nn.trainInsuf)[!names(nn.trainInsuf) %in% c('Variety_Yield')],collapse='+')))
nn.VarietyInsuf = neuralnet(fInsuf,data = nn.trainInsuf, linear.output = F, hidden=2)
nn.PredictInsuf = predict(nn.VarietyInsuf,newdata = nn.testInsuf)*(maxYield-minYield)+ minYield
nn.MSEInsuf = mean((nn.PredictInsuf-nn.laterTest$Variety_Yield)^2)

AvMSE.nn = (sum(nn.MSE, na.rm = TRUE)+nn.MSEInsuf)/84

Model_Test_MSE[7] = AvMSE.nn
```

##Winning Model
```{r}
sqrt(Model_Test_MSE)
##Tree model is the winner
```

#Prescriptive Analytics
```{r}
library(tree)
#Group together ALL variety data points with cluster number = 10
weather_set = newdata[which(newdata$Cluster_num==10),]

#Sample 1000 rows of weather set with replacement
weather_set = weather_set[sample(nrow(weather_set), 1000,replace = TRUE),]
row.names(weather_set)<-NULL
##First the sufficient data sets--> varieties turn numeric
tree.Predict = matrix(nrow = 1000,ncol =length(unique(newdata$Variety)))

for (i in 1:length(unique(newdata$Variety))){
  a = weather_set
  if(length(SplitData[[i]]$Variety) < 35){
    # tree.Predict[,i] = rep(0,1000)
    b = InsufVarietyTrain.df[which(as.numeric(InsufVariety.df$Variety) == InsufVariety.vector[i]),]
    a[,c("Latitude","Longitude","Soil_Type","PH1","Clay1","Sand1","CEC")] = b[1,c("Latitude","Longitude","Soil_Type","PH1","Clay1","Sand1","CEC")]
    tree.Variety= tree(as.formula(paste(target,"~.")),InsufVarietyTrain.df[,c(predictors.treeInsuf,target)])
    set.seed(1)
    crossV.Tree=cv.tree(tree.Variety,FUN=prune.tree)
    best.size = crossV.Tree$size[which(crossV.Tree$dev == min(crossV.Tree$dev))]
    pruned.Tree = prune.tree(tree.Variety,best = max(best.size,2))
    tree.Predict[,i]=predict(pruned.Tree,newdata=a[,predictors.treeInsuf])
  }
  else{
    b = SplitDataTrain[[i]]
    a[,c("Latitude","Longitude","Soil_Type","PH1","Clay1","Sand1","CEC")] = b[1,c("Latitude","Longitude","Soil_Type","PH1","Clay1","Sand1","CEC")]
    tree.Variety= tree(as.formula(paste(target,"~.")),SplitDataTrain[[i]][,c(predictors.treeSuf,target)])
    set.seed(1)
    crossV.Tree=cv.tree(tree.Variety,FUN=prune.tree)
    best.size = crossV.Tree$size[which(crossV.Tree$dev == min(crossV.Tree$dev))]
    pruned.Tree = prune.tree(tree.Variety,best = max(best.size,2))
    tree.Predict[,i]=predict(pruned.Tree,newdata=a[,predictors.treeSuf])
  }
}

#https://www.codingfinance.com/post/2018-05-31-portfolio-opt-in-r/
#remove columns with zero variance
colnames(tree.Predict) <- unique(newdata$Variety)
deleted_columns = which(apply(tree.Predict, 2, var) == 0)
tree.Predict <- tree.Predict[,-as.numeric(which(apply(tree.Predict, 2, var) == 0))]
kept_columns = colnames(tree.Predict)
tree.Predict <- as_tibble(tree.Predict)

row.names(tree.Predict) <- NULL
library(plotly)
library(timetk)

#mean and covariance matrix
mean_yield <- colMeans(tree.Predict)
cov_mat <- cov(tree.Predict)

num_port <- 20000
wts <- matrix(nrow = num_port,ncol = length(kept_columns))
port_yield <- vector('numeric', length = num_port)
port_risk <- vector('numeric', length = num_port)
sharpe_ratio <- vector('numeric', length = num_port)

for (i in seq_along(port_yield)){
  wts[i,] <-runif(n = length(kept_columns))
  wts[i,]<- wts[i,]/sum(wts[i,])
  port_yield[i] <- sum(wts[i,]*mean_yield)
  port_risk[i] <- sqrt(t(wts[i,]) %*% (cov_mat %*% wts[i,]))
  sharpe_ratio[i] <- port_yield[i]/port_risk[i]
}

port_values <- tibble(yield = port_yield,risk = port_risk, SharpeRatio = sharpe_ratio)
colnames(wts) <- kept_columns
port_values<-tk_tbl(cbind(wts, port_values))

min_var <- port_values[which.min(port_values$risk),]
max_sr <- port_values[which.max(port_values$SharpeRatio),]
min_var
max_sr

p <- port_values %>%
  ggplot(aes(x = risk, y = yield, color = sharpe_ratio)) +
  geom_point() +
  theme_classic() +
  labs(x = 'Risk',
       y = 'Yield',
       title = "Portfolio Optimization & Efficient Frontier") +
  geom_point(aes(x = risk,
                 y = yield), data = min_var, color = 'red') +
  geom_point(aes(x =risk,
                 y = yield), data = max_sr, color = 'red') +
 annotate('text', x = 1, y = 56.3, label = "Tangency/Min Var Portfolio",size = 2,color = 'red')

ggplotly(p)
```